


import sys
import os
import jax
import jax.numpy as jnp
import neural_tangents as nt
from neural_tangents import stax
import time

import matplotlib.pyplot as plt
import pandas as pd
import optax
from IPython.display import display 

from typing import Any, Callable, Sequence, Optional, Union

key = jax.random.PRNGKey(0)


# Configure Matplotlib for better inline plots
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
plt.style.use('seaborn-v0_8-whitegrid')


# importing the analyzer class
module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)
from util.ntk_analyzer import NTKAnalyzer
from util.helper import create_mlp_stax, create_mlp_stax_2


import dill
import os

output_directory = "data"





key_data_gen, key_sample = jax.random.split(key, 2)
key = key_data_gen 

N_TRAIN_SAMPLES = 200 # careful, too much could halt your computer

X_domain_min = -1.0
X_domain_max = 1.0

X_train = jnp.linspace(X_domain_min, X_domain_max, N_TRAIN_SAMPLES)[:, None]
Y_train = jnp.sin(3 * jnp.pi * X_train) + 0.8 * jnp.sin(11 * jnp.pi * X_train)

X_test = jnp.linspace(X_domain_min - 0.1, X_domain_max + 0.1, 400)[:, None] 
Y_test_true = jnp.sin(3 * jnp.pi * X_test) + 0.8 * jnp.sin(11 * jnp.pi * X_test)

INPUT_DIM = X_train.shape[1]
OUTPUT_DIM = Y_train.shape[1]

plt.figure(figsize=(12, 6))
plt.plot(X_train, Y_train, label=f'Target Multiscale Function ({N_TRAIN_SAMPLES} points)', color='blue', linewidth=2)
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='upper right')
plt.title('Target Multiscale Function')
plt.grid(True)
plt.show()

print(f"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}")


key, init_key = jax.random.split(key)

HIDDEN_LAYERS = 2      
HIDDEN_WIDTH = 1024     
ACTIVATION_FN_STAX = stax.Erf() 

# for init of weights
W_STD = 1.5            
B_STD = 0.05

init_fn, net_apply_fn, net_stax_kernel_fn = create_mlp_stax(
    depth=HIDDEN_LAYERS, 
    hidden_width=HIDDEN_WIDTH,
    output_dim=OUTPUT_DIM,
    activation_stax_fn=ACTIVATION_FN_STAX,
    W_std=W_STD,
    b_std=B_STD
)

layer_widths_spec = [INPUT_DIM] + [HIDDEN_WIDTH] * HIDDEN_LAYERS + [OUTPUT_DIM]

_, params_initial = init_fn(init_key, (-1, INPUT_DIM)) 

print(f"\nNetwork Architecture: InputDim={INPUT_DIM}, HiddenLayers={HIDDEN_LAYERS}, HiddenWidth={HIDDEN_WIDTH}, OutputDim={OUTPUT_DIM}")
print(f"Layer widths for NTKAnalyzer: {layer_widths_spec}")
print(f"Activation function: {ACTIVATION_FN_STAX}")

Y_pred_initial = net_apply_fn(params_initial, X_test)

plt.figure(figsize=(12, 6))
plt.plot(X_test, Y_test_true, label='True Target Function', color='blue', linestyle='--')
plt.plot(X_train, Y_train, label='Training Data Points', color='blue', alpha=0.5, linewidth=2) 
plt.plot(X_test, Y_pred_initial, label='Initial Network Prediction', color='green', linestyle=':')
plt.title('Initial Network Prediction vs. True Multiscale Function')
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='upper right')
plt.grid(True)
plt.show()


import pandas as pd

ACTIVATION_FUNCTIONS = {
    "ReLU": stax.Relu(),
    "Sigmoid_like": stax.Sigmoid_like(),
    "Gelu": stax.Gelu()
}

CURRENT_ACTIVATION = "Gelu"

network_configurations = [
    # Group 1: Fixed Width, Varying Depth
    # W = 16
    {"name": "D1_W16",   "HIDDEN_LAYERS": 1,   "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/1 = 17.0
    {"name": "D2_W16",   "HIDDEN_LAYERS": 2,   "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/2 = 8.5
    {"name": "D4_W16",   "HIDDEN_LAYERS": 4,   "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/4 = 4.25
    {"name": "D8_W16",   "HIDDEN_LAYERS": 8,   "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/8 = 2.125
    {"name": "D16_W16",  "HIDDEN_LAYERS": 16,  "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/16 = 1.0625
    {"name": "D32_W16",  "HIDDEN_LAYERS": 32,  "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/32 = 0.53125
    {"name": "D48_W16",  "HIDDEN_LAYERS": 48,  "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/48 = 0.3541666666666667
    {"name": "D64_W16",  "HIDDEN_LAYERS": 64,  "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/64 = 0.265625

    # W = 32
    {"name": "D1_W32",   "HIDDEN_LAYERS": 1,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/1 = 33.0
    {"name": "D2_W32",   "HIDDEN_LAYERS": 2,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/2 = 16.5
    {"name": "D3_W32",   "HIDDEN_LAYERS": 3,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/3 = 11.0
    {"name": "D4_W32",   "HIDDEN_LAYERS": 4,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/4 = 8.25
    {"name": "D6_W32",   "HIDDEN_LAYERS": 6,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/6 = 5.5
    {"name": "D8_W32",   "HIDDEN_LAYERS": 8,   "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/8 = 4.125
    {"name": "D10_W32",  "HIDDEN_LAYERS": 10,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/10 = 3.3
    {"name": "D16_W32",  "HIDDEN_LAYERS": 16,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/16 = 2.0625
    {"name": "D24_W32",  "HIDDEN_LAYERS": 24,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/24 = 1.375
    {"name": "D32_W32",  "HIDDEN_LAYERS": 32,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/32 = 1.03125
    {"name": "D40_W32",  "HIDDEN_LAYERS": 40,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/40 = 0.825
    {"name": "D48_W32",  "HIDDEN_LAYERS": 48,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/48 = 0.6875
    {"name": "D64_W32",  "HIDDEN_LAYERS": 64,  "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/64 = 0.515625

    # W = 64
    {"name": "D1_W64",   "HIDDEN_LAYERS": 1,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/1 = 65.0
    {"name": "D2_W64",   "HIDDEN_LAYERS": 2,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/2 = 32.5
    {"name": "D3_W64",   "HIDDEN_LAYERS": 3,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/3 = 21.666666666666668
    {"name": "D4_W64",   "HIDDEN_LAYERS": 4,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/4 = 16.25
    {"name": "D6_W64",   "HIDDEN_LAYERS": 6,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/6 = 10.833333333333334
    {"name": "D8_W64",   "HIDDEN_LAYERS": 8,   "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/8 = 8.125
    {"name": "D12_W64",  "HIDDEN_LAYERS": 12,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/12 = 5.416666666666667
    {"name": "D16_W64",  "HIDDEN_LAYERS": 16,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/16 = 4.0625
    {"name": "D24_W64",  "HIDDEN_LAYERS": 24,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/24 = 2.7083333333333335
    {"name": "D32_W64",  "HIDDEN_LAYERS": 32,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/32 = 2.03125
    {"name": "D48_W64",  "HIDDEN_LAYERS": 48,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/48 = 1.3541666666666667
    {"name": "D64_W64",  "HIDDEN_LAYERS": 64,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/64 = 1.015625
    {"name": "D80_W64",  "HIDDEN_LAYERS": 80,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/80 = 0.8125
    {"name": "D96_W64",  "HIDDEN_LAYERS": 96,  "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/96 = 0.6770833333333334

    # W = 128
    {"name": "D1_W128",  "HIDDEN_LAYERS": 1,   "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/1 = 129.0
    {"name": "D2_W128",  "HIDDEN_LAYERS": 2,   "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/2 = 64.5
    {"name": "D4_W128",  "HIDDEN_LAYERS": 4,   "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/4 = 32.25
    {"name": "D6_W128",  "HIDDEN_LAYERS": 6,   "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/6 = 21.5
    {"name": "D8_W128",  "HIDDEN_LAYERS": 8,   "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/8 = 16.125
    {"name": "D12_W128", "HIDDEN_LAYERS": 12,  "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/12 = 10.75
    {"name": "D16_W128", "HIDDEN_LAYERS": 16,  "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/16 = 8.0625
    {"name": "D32_W128", "HIDDEN_LAYERS": 32,  "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/32 = 4.03125
    {"name": "D64_W128", "HIDDEN_LAYERS": 64,  "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/64 = 2.015625
    {"name": "D96_W128", "HIDDEN_LAYERS": 96,  "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/96 = 1.34375
    {"name": "D128_W128","HIDDEN_LAYERS": 128, "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/128 = 1.0078125

    # W = 256
    {"name": "D1_W256",  "HIDDEN_LAYERS": 1,   "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/1 = 257.0
    {"name": "D2_W256",  "HIDDEN_LAYERS": 2,   "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/2 = 128.5
    {"name": "D4_W256",  "HIDDEN_LAYERS": 4,   "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/4 = 64.25
    {"name": "D8_W256",  "HIDDEN_LAYERS": 8,   "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/8 = 32.125
    {"name": "D16_W256", "HIDDEN_LAYERS": 16,  "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/16 = 16.0625
    {"name": "D32_W256", "HIDDEN_LAYERS": 32,  "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/32 = 8.03125
    {"name": "D64_W256", "HIDDEN_LAYERS": 64,  "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/64 = 4.015625
    {"name": "D96_W256", "HIDDEN_LAYERS": 96,  "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/96 = 2.6770833333333335
    {"name": "D192_W256","HIDDEN_LAYERS": 192, "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/192 = 1.3385416666666667

    # Group 2: Fixed Depth, Varying Width
    # D = 1
    # {"name": "D1_W32",   "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 32,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/1 = 33.0 (duplicate, commented out)
    # {"name": "D1_W64",   "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 64,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/1 = 65.0 (duplicate, commented out)
    # {"name": "D1_W128",  "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 128,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/1 = 129.0 (duplicate, commented out)
    # {"name": "D1_W256",  "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 256,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/1 = 257.0 (duplicate, commented out)
    {"name": "D1_W512",  "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 512,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/1 = 513.0
    {"name": "D1_W1024", "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/1 = 1025.0
    {"name": "D1_W2048", "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/1 = 2049.0
    {"name": "D1_W4096", "HIDDEN_LAYERS": 1, "HIDDEN_WIDTH": 4096, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (4096+1)/1 = 4097.0

    # D = 2
    # {"name": "D2_W16",   "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 16,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/2 = 8.5 (duplicate, commented out)
    # {"name": "D2_W32",   "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 32,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/2 = 16.5 (duplicate, commented out)
    # {"name": "D2_W64",   "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 64,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/2 = 32.5 (duplicate, commented out)
    # {"name": "D2_W128",  "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 128,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/2 = 64.5 (duplicate, commented out)
    # {"name": "D2_W256",  "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 256,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/2 = 128.5 (duplicate, commented out)
    {"name": "D2_W512",  "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 512,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/2 = 256.5
    {"name": "D2_W1024", "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/2 = 512.5
    {"name": "D2_W2048", "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/2 = 1024.5
    {"name": "D2_W4096", "HIDDEN_LAYERS": 2, "HIDDEN_WIDTH": 4096, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (4096+1)/2 = 2048.5

    # D = 4
    # {"name": "D4_W16",   "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 16,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/4 = 4.25 (duplicate, commented out)
    # {"name": "D4_W32",   "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 32,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/4 = 8.25 (duplicate, commented out)
    # {"name": "D4_W64",   "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 64,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/4 = 16.25 (duplicate, commented out)
    # {"name": "D4_W128",  "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 128,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/4 = 32.25 (duplicate, commented out)
    # {"name": "D4_W256",  "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 256,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/4 = 64.25 (duplicate, commented out)
    {"name": "D4_W512",  "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 512,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/4 = 128.25
    {"name": "D4_W1024", "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/4 = 256.25
    {"name": "D4_W2048", "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/4 = 512.25
    {"name": "D4_W4096", "HIDDEN_LAYERS": 4, "HIDDEN_WIDTH": 4096, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (4096+1)/4 = 1024.25

    # D = 8
    # {"name": "D8_W16",   "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 16,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/8 = 2.125 (duplicate, commented out)
    # {"name": "D8_W32",   "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 32,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/8 = 4.125 (duplicate, commented out)
    # {"name": "D8_W64",   "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 64,   "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/8 = 8.125 (duplicate, commented out)
    # {"name": "D8_W128",  "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 128,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/8 = 16.125 (duplicate, commented out)
    # {"name": "D8_W256",  "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 256,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/8 = 32.125 (duplicate, commented out)
    {"name": "D8_W512",  "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 512,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/8 = 64.125
    {"name": "D8_W1024", "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/8 = 128.125
    {"name": "D8_W2048", "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/8 = 256.125
    {"name": "D8_W4096", "HIDDEN_LAYERS": 8, "HIDDEN_WIDTH": 4096, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (4096+1)/8 = 512.125

    # D = 16
    # {"name": "D16_W16",  "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 16,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (16+1)/16 = 1.0625 (duplicate, commented out)
    # {"name": "D16_W32",  "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 32,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (32+1)/16 = 2.0625 (duplicate, commented out)
    # {"name": "D16_W64",  "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 64,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (64+1)/16 = 4.0625 (duplicate, commented out)
    # {"name": "D16_W128", "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 128, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (128+1)/16 = 8.0625 (duplicate, commented out)
    # {"name": "D16_W256", "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 256, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (256+1)/16 = 16.0625 (duplicate, commented out)
    {"name": "D16_W512", "HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 512, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/16 = 32.0625
    {"name": "D16_W1024","HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/16 = 64.0625
    {"name": "D16_W2048","HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/16 = 128.0625
    {"name": "D16_W4096","HIDDEN_LAYERS": 16, "HIDDEN_WIDTH": 4096, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (4096+1)/16 = 256.0625

    # D = 32
    {"name": "D32_W512", "HIDDEN_LAYERS": 32, "HIDDEN_WIDTH": 512,  "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (512+1)/32 = 16.03125
    {"name": "D32_W1024","HIDDEN_LAYERS": 32, "HIDDEN_WIDTH": 1024, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (1024+1)/32 = 32.03125
    {"name": "D32_W2048","HIDDEN_LAYERS": 32, "HIDDEN_WIDTH": 2048, "ACTIVATION_FN_STAX": ACTIVATION_FUNCTIONS[CURRENT_ACTIVATION], "ACTIVATION_NAME": CURRENT_ACTIVATION, "W_STD": jnp.sqrt(2.0), "B_STD": 0.0}, # beta = (2048+1)/32 = 64.03125
]

# Choose a single data point from X_train (defined in Cell 2)
x_single_point_for_analysis = X_train[0:1, :] # Shape (1, INPUT_DIM)
x_norm_sq = jnp.linalg.norm(x_single_point_for_analysis.squeeze())**2 
n0 = INPUT_DIM # Input dimension

# For verifying Theorem 2
y_star_for_sgd_step = Y_train[0:1, :] # Target for the chosen x_single_point
learning_rate_sgd = 1e-3 # Learning rate for the single SGD step

num_initializations_for_verification = 100

print(f"Will analyze {len(network_configurations)} network configurations.")
print(f"Using x_single_point: {x_single_point_for_analysis.squeeze()}, ||x||^2: {x_norm_sq:.4f}")
print(f"Input dimension n0 (INPUT_DIM): {n0}")
print(f"Number of random initializations for averaging: {num_initializations_for_verification}")

# List to store results from all configurations
relu_result = []

# Main PRNG key for this loop of experiments
main_exp_key = jax.random.PRNGKey(789)

def mse_loss_single_point_fn(params_current, x_batch, y_batch, apply_fn_local):
    y_pred = apply_fn_local(params_current, x_batch)
    if y_pred.shape != y_batch.shape and y_pred.shape[-1] == y_batch.shape[-1]:
        y_pred = y_pred.reshape(y_batch.shape)
    return 0.5 * jnp.sum((y_pred - y_batch)**2) # Sum for scalar loss








for config_idx, config in enumerate(network_configurations):
    print(f"\n--- Processing Configuration {config_idx+1}/{len(network_configurations)}: {config['name']} ---")
    start_time_config = time.time()

    current_hidden_layers = config["HIDDEN_LAYERS"]
    current_hidden_width = config["HIDDEN_WIDTH"]
    current_activation_fn = config["ACTIVATION_FN_STAX"]
    current_w_std = config["W_STD"]
    current_b_std = config["B_STD"]

    init_fn_config, apply_fn_config, _, layer_widths_config = create_mlp_stax_2(
        depth_hidden=current_hidden_layers,
        hidden_width=current_hidden_width,
        output_dim=OUTPUT_DIM, 
        input_dim=INPUT_DIM,   
        activation_stax_fn=current_activation_fn,
        W_std=current_w_std,
        b_std=current_b_std
    )

    effective_depth_d_config = current_hidden_layers + 1
    
    beta = None
    if layer_widths_config and len(layer_widths_config) > 2:
        hidden_widths_config = layer_widths_config[1:-1]
        if hidden_widths_config:
            beta = sum(1.0 / w for w in hidden_widths_config if w > 0)
    
    beta_print_str = f"{beta:.4f}" if beta is not None else "N/A"
    print(f"  Config: Hidden Layers={current_hidden_layers}, Width={current_hidden_width}, Beta={beta_print_str}")

    k_xx_values_current_config = []
    delta_k_xx_values_current_config = []

    config_prng_key, main_exp_key = jax.random.split(main_exp_key) 

    for i_init in range(num_initializations_for_verification):
        iter_key, config_prng_key = jax.random.split(config_prng_key) 
        _, params_t0 = init_fn_config(iter_key, (-1, INPUT_DIM))
        
        analyzer = NTKAnalyzer(
            apply_fn=apply_fn_config, 
            params=params_t0,
            depth=current_hidden_layers, 
            widths=layer_widths_config
        )
        
        diag_val_obj_t0 = analyzer.get_ntk_diagonal_per_output(x_single_point_for_analysis)
        if diag_val_obj_t0 is None:
            print(f"    Warning: Could not get K(x,x) for init {i_init+1}")
            continue 
        
        k_t0_xx = diag_val_obj_t0[0, 0].item()
        k_xx_values_current_config.append(k_t0_xx)
        
        loss_fn_single_for_grad_config = jax.tree_util.Partial(mse_loss_single_point_fn, apply_fn_local=apply_fn_config)
        grad_fn_single_config = jax.grad(loss_fn_single_for_grad_config)

        grads_t0 = grad_fn_single_config(params_t0, x_single_point_for_analysis, y_star_for_sgd_step)
        params_t1 = jax.tree_util.tree_map(lambda p, g: p - learning_rate_sgd * g, params_t0, grads_t0)
        
        diag_val_obj_t1 = analyzer.get_ntk_diagonal_per_output(x_single_point_for_analysis, params_override=params_t1)
        if diag_val_obj_t1 is None:
            print(f"    Warning: Could not get K(x,x) after SGD step for init {i_init+1}")
            continue

        k_t1_xx = diag_val_obj_t1[0, 0].item()
        delta_k_xx_values_current_config.append(k_t1_xx - k_t0_xx)

    config_results = {"config_name": config["name"], "config_details": config, "beta": beta}
    
    if k_xx_values_current_config:
        k_xx_arr = jnp.array(k_xx_values_current_config)
        emp_E_Kxx = jnp.mean(k_xx_arr)
        emp_E_Kxx_sq = jnp.mean(k_xx_arr**2)
        emp_Var_Kxx = jnp.var(k_xx_arr)
        
        config_results["emp_E_Kxx"] = emp_E_Kxx.item()
        config_results["emp_E_Kxx_sq"] = emp_E_Kxx_sq.item()
        config_results["emp_Var_Kxx"] = emp_Var_Kxx.item()
        
        if emp_E_Kxx**2 != 0:
            emp_norm_sq_moment = emp_E_Kxx_sq / (emp_E_Kxx**2)
            config_results["emp_norm_sq_moment"] = emp_norm_sq_moment.item() 
            config_results["emp_sq_coeff_var"] = emp_norm_sq_moment.item() - 1 
        else:
            config_results["emp_norm_sq_moment"] = jnp.nan
            config_results["emp_sq_coeff_var"] = jnp.nan

        is_relu_activation_config = (config.get("ACTIVATION_NAME") == "ReLU")
        
        bias_contrib = 0.5 if config["B_STD"] > 1e-6 and is_relu_activation_config else 0.0

        # compute theoritical results
        theo_E_Kxx_relu = effective_depth_d_config * (bias_contrib + x_norm_sq / n0)
        config_results["theo_E_Kxx_relu_formula"] = theo_E_Kxx_relu.item()
        print(f"  Empirical E[K(x,x)]: {emp_E_Kxx:.4f} (Theoretical for ReLU: {theo_E_Kxx_relu:.4f})")
    else:
        print("  No K(x,x) values collected for this config.")

    if delta_k_xx_values_current_config:
        delta_k_xx_arr = jnp.array(delta_k_xx_values_current_config)
        emp_E_Delta_Kxx = jnp.mean(delta_k_xx_arr)
        config_results["emp_E_Delta_Kxx"] = emp_E_Delta_Kxx.item()
        print(f"  Empirical E[Delta K(x,x)]: {emp_E_Delta_Kxx:.6e}")
        if "emp_E_Kxx" in config_results and config_results["emp_E_Kxx"] !=0 :
            emp_relative_delta_K = emp_E_Delta_Kxx / config_results["emp_E_Kxx"]
            config_results["emp_relative_delta_K"] = emp_relative_delta_K.item()
            print(f"  Empirical Relative E[Delta K]/E[K]: {emp_relative_delta_K:.6e}")
            C_factor_hn = 5.0 # for ReLU
            if beta is not None and is_relu_activation_config : 
                 theoretical_scaling_term_delta_K = effective_depth_d_config * beta / n0 * jnp.exp(C_factor_hn * beta) * learning_rate_sgd
                 config_results["theo_scaling_term_delta_K_relu"] = theoretical_scaling_term_delta_K.item()
                 print(f"    Theoretical scaling term for Delta K (d*beta/n0 * exp(C*beta) * lr, C={C_factor_hn}): {theoretical_scaling_term_delta_K:.4e}")
    else:
        print("  No Delta K(x,x) values collected for this config.")
        
        
    relu_result.append(config_results)
    end_time_config = time.time()
    print(f"  Finished processing config {config['name']} in {end_time_config - start_time_config:.2f} seconds.")

loaded_results_df = pd.DataFrame(relu_result)
print("\n--- Summary of Results ---")
# display(loaded_results_df)


if not os.path.exists(output_directory):
    os.makedirs(output_directory)
    print(f"Created directory: {output_directory}")

base_filename = "gelu-24-05-01.dill.pkl"
pickle_filename = os.path.join(output_directory, base_filename)

with open(pickle_filename, 'wb') as f:
    dill.dump(loaded_results_df, f)
print(f"Results saved to {pickle_filename}")





base_filename = "relu.dill.pkl"
pickle_filename = os.path.join(output_directory, base_filename)

loaded_results_df = None
try:
    with open(pickle_filename, 'rb') as f:
        loaded_results_df = dill.load(f)
except Exception as e:
    print(e)

relu_result = loaded_results_df
display(relu_result)











# --- Plot for Stochasticity: E[K^2]/E[K]^2 vs. beta ---
plot_data_stochasticity = []
# Iterate over DataFrame rows as dictionaries
for res_stoch in relu_result.to_dict('records'): 
    beta_val = res_stoch.get("beta")
    emp_norm_sq_moment_val = res_stoch.get("emp_norm_sq_moment")
    
    if beta_val is not None and \
       emp_norm_sq_moment_val is not None and \
       not (isinstance(emp_norm_sq_moment_val, float) and pd.isna(emp_norm_sq_moment_val)): # Use pd.isna for DataFrames
        
        activation_name_stoch = "Unknown"
        config_details_stoch = res_stoch.get("config_details")
        if isinstance(config_details_stoch, dict):
            activation_name_stoch = config_details_stoch.get("ACTIVATION_NAME", "Unknown")

        plot_data_stochasticity.append({
            "beta": beta_val, 
            "emp_norm_sq_moment": emp_norm_sq_moment_val,
            "config_name": res_stoch.get("config_name", "N/A"),
            "activation_name": activation_name_stoch
        })

plot_df_stochasticity = pd.DataFrame(plot_data_stochasticity)
plot_df_stochasticity = plot_df_stochasticity.sort_values(by="beta")

plt.figure(figsize=(14, 6))

plt.plot(plot_df_stochasticity["beta"].values, 
         plot_df_stochasticity["emp_norm_sq_moment"].values, 
         "o-", markersize=5, alpha=0.6, label="Empirical E[K^2]/E[K]^2")

relu_plot_df_stoch = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == "ReLU"]
if not relu_plot_df_stoch.empty:
    C_factor_hn = 5.0 
    betas_for_relu_plot_stoch = jnp.array(relu_plot_df_stoch["beta"].values)
    if betas_for_relu_plot_stoch.size > 0:
        theo_scaling_relu = jnp.exp(C_factor_hn * betas_for_relu_plot_stoch)
        plt.plot(betas_for_relu_plot_stoch, 
                 theo_scaling_relu, 
                 "--", color='red', 
                 label=f"Theoretical exp({C_factor_hn}*beta) (ReLU)")

plt.xlabel("β")
plt.ylabel("E[K(x,x)^2] / (E[K(x,x)])^2")
plt.title("Normalized Second Moment for ReLU")
plt.legend()
plt.grid(True, which="both")
plt.yscale('log')

plt.tight_layout()
plt.show()





# this loads the files

files = {
    "ReLU": "relu.dill.pkl",
    "GeLU": "gelu.dill.pkl",
    "sigmoid": "sigmoid.dill.pkl"
}

plot_data_stochasticity = []
for activation, filename in files.items():
    pickle_filename = os.path.join(output_directory, filename)
    loaded_results_df = None
    try:
        with open(pickle_filename, 'rb') as f:
            loaded_results_df = dill.load(f)
    except Exception as e:
        print(f"Error loading {filename}: {e}")
        continue
        
    for res_stoch in loaded_results_df.to_dict('records'):
        beta_val = res_stoch.get("beta")
        emp_norm_sq_moment_val = res_stoch.get("emp_norm_sq_moment")
        
        if (beta_val is not None and
            emp_norm_sq_moment_val is not None and
            not (isinstance(emp_norm_sq_moment_val, float) and pd.isna(emp_norm_sq_moment_val))):
            
            activation_name_stoch = activation 
            config_details_stoch = res_stoch.get("config_details")
            if isinstance(config_details_stoch, dict):
                # Override with config_details if available
                activation_name_stoch = config_details_stoch.get("ACTIVATION_NAME", activation)

            plot_data_stochasticity.append({
                "beta": beta_val,
                "emp_norm_sq_moment": emp_norm_sq_moment_val,
                "config_name": res_stoch.get("config_name", "N/A"),
                "activation_name": activation_name_stoch
            })

plot_df_stochasticity = pd.DataFrame(plot_data_stochasticity)
plot_df_stochasticity = plot_df_stochasticity.sort_values(by="beta")

display(plot_df_stochasticity)


activation_styles = {
    "ReLU": {"color": "red", "marker": "o", "label": "ReLU"},
    "Gelu": {"color": "blue", "marker": "^", "label": "GeLU"},
    "Sigmoid_like": {"color": "green", "marker": "s", "label": "sigmoid"}
}

# Plotting
plt.figure(figsize=(14, 6))

# Subplot 1: Normalized Second Moment
for activation in activation_styles:
    df_subset = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == activation]
    if not df_subset.empty:
        plt.plot(
            df_subset["beta"].values,
            df_subset["emp_norm_sq_moment"].values,
            marker=activation_styles[activation]["marker"],
            color=activation_styles[activation]["color"],
            markersize=5,
            alpha=0.6,
            label=f"Empirical E[K^2]/E[K]^2 ({activation_styles[activation]['label']})"
        )

# Add theoretical curve for ReLU
relu_plot_df_stoch = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == "ReLU"]
if not relu_plot_df_stoch.empty:
    C_factor_hn = 5.0
    betas_for_relu_plot_stoch = jnp.array(relu_plot_df_stoch["beta"].values)
    if betas_for_relu_plot_stoch.size > 0:
        theo_scaling_relu = jnp.exp(C_factor_hn * betas_for_relu_plot_stoch)
        plt.plot(
            betas_for_relu_plot_stoch,
            theo_scaling_relu,
            "--",
            color='darkred',
            label=f"Theoretical exp({C_factor_hn}*beta) (ReLU)"
        )

plt.xlabel("β")
plt.ylabel("E[K(x,x)^2] / (E[K(x,x)])^2")
plt.title("Normalized Second Moment for ReLU, GeLU, Sigmoid")
plt.legend()
plt.grid(True, which="both")
plt.yscale('log')

plt.tight_layout()
plt.show()


plt.figure(figsize=(14, 6))
BETA_THRES = 0.5

for activation in activation_styles:
    df_subset = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == activation]
    df_subset_zoomed = df_subset[df_subset["beta"] <= BETA_THRES]
    if not df_subset_zoomed.empty:
        plt.plot(
            df_subset_zoomed["beta"].values,
            df_subset_zoomed["emp_norm_sq_moment"].values,
            marker=activation_styles[activation]["marker"],
            color=activation_styles[activation]["color"],
            markersize=5,
            alpha=0.6,
            label=f"Empirical E[K^2]/E[K]^2 ({activation_styles[activation]['label']})"
        )

# Add theoretical curve for ReLU (zoomed)
relu_plot_df_stoch = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == "ReLU"]
relu_plot_df_stoch_zoomed = relu_plot_df_stoch[relu_plot_df_stoch["beta"] <= 1.5]
if not relu_plot_df_stoch_zoomed.empty:
    C_factor_hn = 5.0
    betas_for_relu_plot_stoch = jnp.array(relu_plot_df_stoch_zoomed["beta"].values)
    if betas_for_relu_plot_stoch.size > 0:
        theo_scaling_relu = jnp.exp(C_factor_hn * betas_for_relu_plot_stoch)
        plt.plot(
            betas_for_relu_plot_stoch,
            theo_scaling_relu,
            "--",
            color='darkred',
            label=f"Theoretical exp({C_factor_hn}*beta) (ReLU)"
        )

plt.xlabel("β")
plt.ylabel("E[K(x,x)^2] / (E[K(x,x)])^2")
plt.title(f"Normalized Second Moment (β <= {BETA_THRES})")
plt.legend()
plt.grid(True, which="both")
plt.yscale('log')
plt.xlim(0, BETA_THRES)

plt.tight_layout()
plt.show()


activations = ["ReLU", "Gelu", "Sigmoid_like"]

for activation in activations:
    df_subset = plot_df_stochasticity[plot_df_stochasticity["activation_name"] == activation]
    if not df_subset.empty:
        activation_max = df_subset["emp_norm_sq_moment"].max()
        activation_min = df_subset["emp_norm_sq_moment"].min()
    else:
        activation_max = activation_min = None
    
    print(f"{activation} Values:")
    print(f"Max: {activation_max}")
    print(f"Min: {activation_min}")
    print()








# --- Plot for NTK Evolution: E[Delta K]/E[K] vs theoretical scaling term ---
print("\n--- Plotting NTK Evolution: Relative Change vs. Theoretical Scaling ---")
plot_data_delta_k = []
# Iterate over DataFrame rows as dictionaries
for res_delta in relu_result.to_dict('records'): 
    activation_name_delta = "Unknown"
    config_details_delta = res_delta.get("config_details")
    if isinstance(config_details_delta, dict):
        activation_name_delta = config_details_delta.get("ACTIVATION_NAME", "Unknown")
    
    is_relu_config_delta = (activation_name_delta == "ReLU")

    beta_dk = res_delta.get("beta")
    emp_rel_dk = res_delta.get("emp_relative_delta_K")
    theo_scale_dk = res_delta.get("theo_scaling_term_delta_K_relu")

    if beta_dk is not None and \
       emp_rel_dk is not None and \
       not (isinstance(emp_rel_dk, float) and pd.isna(emp_rel_dk)) and \
       theo_scale_dk is not None and \
       is_relu_config_delta: 
        plot_data_delta_k.append({
            "beta": beta_dk,
            "emp_relative_delta_K": emp_rel_dk,
            "theo_scaling_term_delta_K_relu": theo_scale_dk or 0,
            "config_name": res_delta.get("config_name", "N/A")
        })
plot_df_delta_k = pd.DataFrame(plot_data_delta_k)

print(f"Number of ReLU configurations with valid data for Delta K plot: {len(plot_df_delta_k)}")
if not plot_df_delta_k.empty:
    print("Data for Delta K plot (first 5 rows):")
    from IPython.display import display
    display(plot_df_delta_k.head())

    plot_df_delta_k = plot_df_delta_k.sort_values(by="theo_scaling_term_delta_K_relu")
    
    x_plot_values_dk = jnp.asarray(plot_df_delta_k["theo_scaling_term_delta_K_relu"].values)
    y_plot_values_dk_abs = jnp.abs(jnp.asarray(plot_df_delta_k["emp_relative_delta_K"].values))
    config_names_dk_plot = plot_df_delta_k["config_name"].values

    valid_log_indices_dk_x = x_plot_values_dk > 1e-9 
    valid_log_indices_dk_y = y_plot_values_dk_abs > 1e-9 
    valid_log_indices_dk = valid_log_indices_dk_x & valid_log_indices_dk_y
    
    x_plot_final_dk = x_plot_values_dk[valid_log_indices_dk]
    y_plot_final_dk = y_plot_values_dk_abs[valid_log_indices_dk]
    names_final_dk = config_names_dk_plot[valid_log_indices_dk]

    if x_plot_final_dk.size > 0:
        plt.figure(figsize=(10, 7)) 
        plt.scatter(x_plot_final_dk, 
                    y_plot_final_dk, 
                    label="Abs(Empirical Relative E[Delta K]/E[K]) (ReLU)",
                    s=50) 
        
        # for i, txt in enumerate(names_final_dk):
        #     plt.annotate(txt, (x_plot_final_dk[i], y_plot_final_dk[i]),
        #                  textcoords="offset points", xytext=(0,10), ha='center', fontsize=9)

        valid_theo_dk = x_plot_final_dk[jnp.isfinite(x_plot_final_dk)]
        valid_emp_dk = y_plot_final_dk[jnp.isfinite(y_plot_final_dk)]

        if valid_theo_dk.size > 0 and valid_emp_dk.size > 0:
            min_val_plot = min(valid_theo_dk.min().item(), valid_emp_dk.min().item())
            max_val_plot = max(valid_theo_dk.max().item(), valid_emp_dk.max().item())
        
            if not (jnp.isnan(min_val_plot) or jnp.isnan(max_val_plot) or 
                    jnp.isinf(min_val_plot) or jnp.isinf(max_val_plot) or
                    abs(min_val_plot - max_val_plot) < 1e-9): 
                plt.plot([min_val_plot, max_val_plot], [min_val_plot, max_val_plot], 
                         'r--', alpha=0.7, label="y=x (Proportionality Guide)")

        plt.xlabel("Theoretical Scaling Term for Delta K (ReLU)")
        plt.ylabel("Abs(Empirical Relative E[∆ K]/E[K])")
        plt.title("NTK Evolution: Magnitude of Relative Change vs. Theoretical Scaling")
        plt.xscale('log')
        plt.yscale('log')
        plt.legend()
        plt.grid(True, which="both")
        plt.show()
    else:
        print("No positive empirical relative Delta K values (or positive theoretical scaling terms) available to plot on log scale for Delta K.")
else:
    print("No ReLU data available to plot NTK evolution scaling vs theoretical term (plot_df_delta_k is empty).")





# this loads the files

files = {
    "ReLU": "relu.dill.pkl",
    "GeLU": "gelu.dill.pkl",
    "sigmoid": "sigmoid.dill.pkl"
}

plot_data_delta_k = []
for activation, filename in files.items():
    pickle_filename = os.path.join(output_directory, filename)
    loaded_results_df = None
    try:
        with open(pickle_filename, 'rb') as f:
            loaded_results_df = dill.load(f)
    except Exception as e:
        print(f"Error loading {filename}: {e}")
        continue
        
    for res_delta in loaded_results_df.to_dict('records'):
        activation_name_delta = "Unknown"
        config_details_delta = res_delta.get("config_details")
        if isinstance(config_details_delta, dict):
            activation_name_delta = config_details_delta.get("ACTIVATION_NAME", "Unknown")
                
        beta_dk = res_delta.get("beta")
        emp_rel_dk = res_delta.get("emp_relative_delta_K")
    
        if beta_dk is not None and \
           emp_rel_dk is not None and \
           not (isinstance(emp_rel_dk, float) and pd.isna(emp_rel_dk)) and \
           theo_scale_dk is not None:
            plot_data_delta_k.append({
                "beta": beta_dk,
                "emp_relative_delta_K": emp_rel_dk,
                "config_name": res_delta.get("config_name", "N/A"),
                "activation_name": activation_name_delta,
                "config_details": config_details_delta,
            })

plot_df_delta_k = pd.DataFrame(plot_data_delta_k)
plot_df_delta_k = plot_df_delta_k.sort_values(by="beta")

display(plot_df_delta_k)


activation_styles = {
    "ReLU": {"color": "red", "marker": "o", "label": "ReLU"},
    "Gelu": {"color": "blue", "marker": "^", "label": "GeLU"},
    "Sigmoid_like": {"color": "green", "marker": "s", "label": "Sigmoid"}
}

plot_data_delta_k = plot_df_delta_k.copy()

plot_data_delta_k['depth'] = plot_data_delta_k['config_name'].str.extract(r'D(\d+)_W(\d+)').iloc[:, 0].astype(float)
plot_data_delta_k['width'] = plot_data_delta_k['config_name'].str.extract(r'D(\d+)_W(\d+)').iloc[:, 1].astype(float)

# Extract input dimension (n0) from config_details if available, or assume a default (e.g., 1 if not specified)
plot_data_delta_k['n0'] = plot_data_delta_k['config_details'].apply(
    lambda x: x.get('INPUT_DIM', 1) if isinstance(x, dict) else 1
).astype(float)

# Compute beta = d/n for each configuration
plot_data_delta_k['beta'] = plot_data_delta_k.apply(
    lambda row: row['depth'] / row['width'] if row['depth'] is not None and row['width'] is not None and row['width'] != 0 else None,
    axis=1
)

# Filter out rows where beta or emp_relative_delta_K is None or NaN
plot_data_delta_k = plot_data_delta_k.dropna(subset=['beta', 'emp_relative_delta_K'])

# Sort by beta for consistent plotting
plot_data_delta_k = plot_data_delta_k.sort_values(by="beta")

# Extract plot values
beta_values = jnp.asarray(plot_data_delta_k["beta"].values)
y_plot_values_dk_abs = jnp.abs(jnp.asarray(plot_data_delta_k["emp_relative_delta_K"].values))
config_names_dk_plot = plot_data_delta_k["config_name"].values
activation_names_dk = plot_data_delta_k["activation_name"].values
depth_values = jnp.asarray(plot_data_delta_k["depth"].values)
n0_values = jnp.asarray(plot_data_delta_k["n0"].values)

# Filter for valid log-scale values (positive and non-zero)
valid_log_indices_beta = beta_values > 1e-9
valid_log_indices_y = y_plot_values_dk_abs > 1e-9
valid_log_indices = valid_log_indices_beta & valid_log_indices_y

beta_final = beta_values[valid_log_indices]
y_plot_final_dk = y_plot_values_dk_abs[valid_log_indices]
names_final_dk = config_names_dk_plot[valid_log_indices]
activations_final_dk = activation_names_dk[valid_log_indices]
depth_final = depth_values[valid_log_indices]
n0_final = n0_values[valid_log_indices]

plt.figure(figsize=(15, 7))

# Plot empirical data for each activation function
for activation in activation_styles.keys():
    # Filter data for the current activation
    mask = activations_final_dk == activation
    if not jnp.any(mask):
        print(f"No valid data for activation {activation}")
        continue

    beta_activation = beta_final[mask]
    y_activation = y_plot_final_dk[mask]
    names_activation = names_final_dk[mask]

    # Plot scatter points with specified style
    style = activation_styles.get(activation, {"color": "black", "marker": ".", "label": activation})
    plt.scatter(beta_activation, y_activation, 
                color=style["color"], marker=style["marker"], label=style["label"], s=50)

    # Annotate points with config names -> this makes it not very readable
    # for i, txt in enumerate(names_activation):
    #     plt.annotate(txt, (beta_activation[i], y_activation[i]),
    #                  textcoords="offset points", xytext=(0,10), ha='center', fontsize=9)

# Plot theoretical curve for ReLU: (d * beta / n_0) * exp(5 * beta)
# Generate a range of beta values for the theoretical curve
beta_range = jnp.linspace(beta_final.min(), beta_final.max(), 100)
# Use median values from ReLU configurations for consistency
relu_mask = activations_final_dk == "ReLU"
if jnp.any(relu_mask):
    median_depth = jnp.median(depth_final[relu_mask])
    median_n0 = jnp.median(n0_final[relu_mask])
    theoretical_scaling = (median_depth * beta_range / median_n0) * jnp.exp(5 * beta_range) * learning_rate_sgd
    plt.plot(beta_range, theoretical_scaling, 'r-', label=f'Theoretical ReLU Scaling', alpha=0.7) # (d*beta/n0)exp(5*beta)

# Configure plot settings
plt.xlabel("Beta (d/n)")
plt.ylabel("Abs(Empirical Relative E[∆ K]/E[K])")
plt.title("NTK Evolution: Magnitude of Relative Change vs. Beta (d/n) for All Activations")
plt.xscale('log')
plt.yscale('log')
plt.legend(bbox_to_anchor=(1.03, 1), loc='upper left', title="Activation Function")
plt.tight_layout(rect=[0, 0, 0.9, 1])
plt.show()








EPOCHS_FULL_TRAINING = 1000 
LEARNING_RATE_FULL_TRAINING = 1e-3
BATCH_SIZE_FULL_TRAINING = 128 
TRACK_NTK_EVERY_EPOCHS = EPOCHS_FULL_TRAINING // 20 # Track NTK at these epoch intervals
if TRACK_NTK_EVERY_EPOCHS == 0:
    TRACK_NTK_EVERY_EPOCHS = 1

def mse_loss_for_training(params_current, x_batch, y_batch, apply_fn_local):
    y_pred = apply_fn_local(params_current, x_batch)
    if hasattr(y_batch, 'shape') and hasattr(y_pred, 'shape') and y_pred.shape != y_batch.shape:
        if y_pred.ndim == y_batch.ndim + 1 and y_pred.shape[-1] == 1:
            y_pred = y_pred.squeeze(axis=-1)
        elif y_batch.ndim == y_pred.ndim + 1 and y_batch.shape[-1] == 1:
             y_batch = y_batch.squeeze(axis=-1)

        if y_pred.shape != y_batch.shape:
            y_pred = y_pred.reshape(y_batch.shape) # Fallback to original reshape
    return jnp.mean((y_pred - y_batch)**2)

results_ntk_during_full_training = []
main_train_loop_key = jax.random.PRNGKey(1234)

print(f"--- Starting NTK Tracking during Full Training ---")
print(f"Training for {EPOCHS_FULL_TRAINING} epochs, BATCH_SIZE={BATCH_SIZE_FULL_TRAINING}")
print(f"NTK (K(x,x) at fixed point) will be tracked every {TRACK_NTK_EVERY_EPOCHS} epochs.")

for config_idx, config in enumerate(network_configurations):
    print(f"\n--- Processing Configuration {config_idx+1}/{len(network_configurations)}: {config['name']} ---")
    config_start_time = time.time()

    # Network setup from configuration
    current_hidden_layers = config["HIDDEN_LAYERS"]
    current_hidden_width = config["HIDDEN_WIDTH"]
    current_activation_fn = config["ACTIVATION_FN_STAX"]
    current_w_std = config["W_STD"]
    current_b_std = config["B_STD"]

    init_fn_config, apply_fn_config, _, layer_widths_config = create_mlp_stax_2(
        depth_hidden=current_hidden_layers,
        hidden_width=current_hidden_width,
        output_dim=OUTPUT_DIM, # Assumed global
        input_dim=INPUT_DIM,   # Assumed global
        activation_stax_fn=current_activation_fn,
        W_std=current_w_std,
        b_std=current_b_std
    )

    temp_beta_val = None
    if layer_widths_config and len(layer_widths_config) > 2:
        hidden_widths_conf = layer_widths_config[1:-1]
        if hidden_widths_conf:
            temp_beta_val = sum(1.0 / w for w in hidden_widths_conf if w > 0)
    beta_print_str = f"{temp_beta_val:.4f}" if temp_beta_val is not None else "N/A"
    print(f"  Config: {config['name']}, Beta={beta_print_str}")

    # Initialize parameters
    main_train_loop_key, init_key = jax.random.split(main_train_loop_key)
    _, params_initial_training = init_fn_config(init_key, (-1, INPUT_DIM))

    # Optimizer setup
    optimizer = optax.adam(LEARNING_RATE_FULL_TRAINING)
    opt_state_initial_training = optimizer.init(params_initial_training)

    # Loss function specific to this network's apply_fn
    loss_fn_for_grad_training = jax.tree_util.Partial(mse_loss_for_training, apply_fn_local=apply_fn_config)

    @jax.jit
    def train_step_jit(params_current, opt_state_current, x_batch, y_batch):
        loss_value, grads = jax.value_and_grad(loss_fn_for_grad_training)(params_current, x_batch, y_batch)
        updates, opt_state_new = optimizer.update(grads, opt_state_current, params_current)
        params_new = optax.apply_updates(params_current, updates)
        return params_new, opt_state_new, loss_value

    # NTK Tracking
    ntk_evolution_for_this_config = []
    params_current_training = params_initial_training
    opt_state_current_training = opt_state_initial_training

    # Instantiate NTKAnalyzer (can be done once per config)
    # The params used for instantiation don't matter if we always use params_override
    analyzer_for_training = NTKAnalyzer(
        apply_fn=apply_fn_config,
        params=params_initial_training, # Placeholder, will be overridden
        depth=current_hidden_layers,
        widths=layer_widths_config
    )

    # --- Track NTK at Epoch 0 (before training) ---
    diag_val_obj_epoch0 = analyzer_for_training.get_ntk_diagonal_per_output(x_single_point_for_analysis, params_override=params_current_training)
    k_xx_epoch0_val = jnp.nan # Default
    if diag_val_obj_epoch0 is not None:
        k_xx_epoch0_val = diag_val_obj_epoch0[0, 0].item()
    ntk_evolution_for_this_config.append({"epoch": 0, "k_xx": k_xx_epoch0_val, "loss": jnp.nan}) # Loss before training
    print(f"  Epoch 0: K(x,x) = {k_xx_epoch0_val:.6e}")

    # --- Training Loop ---
    num_batches_per_epoch = N_TRAIN_SAMPLES // BATCH_SIZE_FULL_TRAINING if N_TRAIN_SAMPLES > BATCH_SIZE_FULL_TRAINING and BATCH_SIZE_FULL_TRAINING > 0 else 1

    for epoch in range(EPOCHS_FULL_TRAINING):
        main_train_loop_key, perm_key_epoch = jax.random.split(main_train_loop_key)
        epoch_loss_sum = 0.0
        
        if BATCH_SIZE_FULL_TRAINING > 0 and BATCH_SIZE_FULL_TRAINING < N_TRAIN_SAMPLES:
            perm = jax.random.permutation(perm_key_epoch, N_TRAIN_SAMPLES)
            shuffled_X_train = X_train[perm]
            shuffled_Y_train = Y_train[perm]
            
            for i_batch in range(num_batches_per_epoch):
                start = i_batch * BATCH_SIZE_FULL_TRAINING
                end = start + BATCH_SIZE_FULL_TRAINING
                x_batch = shuffled_X_train[start:end]
                y_batch = shuffled_Y_train[start:end]
                params_current_training, opt_state_current_training, loss_val_batch = train_step_jit(params_current_training, opt_state_current_training, x_batch, y_batch)
                epoch_loss_sum += loss_val_batch
            current_epoch_avg_loss = (epoch_loss_sum / num_batches_per_epoch).item()
        else: # Full batch
            params_current_training, opt_state_current_training, loss_val_epoch = train_step_jit(params_current_training, opt_state_current_training, X_train, Y_train)
            current_epoch_avg_loss = loss_val_epoch.item()

        # --- Track NTK Periodically ---
        if (epoch + 1) % TRACK_NTK_EVERY_EPOCHS == 0 or (epoch + 1) == EPOCHS_FULL_TRAINING:
            diag_val_obj_current_epoch = analyzer_for_training.get_ntk_diagonal_per_output(x_single_point_for_analysis, params_override=params_current_training)
            k_xx_current_epoch_val = jnp.nan
            if diag_val_obj_current_epoch is not None:
                k_xx_current_epoch_val = diag_val_obj_current_epoch[0, 0].item()
            ntk_evolution_for_this_config.append({"epoch": epoch + 1, "k_xx": k_xx_current_epoch_val, "loss": current_epoch_avg_loss})
            if (epoch + 1) % (TRACK_NTK_EVERY_EPOCHS * 5) == 0 or (epoch + 1) == EPOCHS_FULL_TRAINING : # Print less frequently
                 print(f"  Epoch {epoch+1}/{EPOCHS_FULL_TRAINING}: Loss = {current_epoch_avg_loss:.6f}, K(x,x) = {k_xx_current_epoch_val:.6e}")
    
    # Store results for this configuration
    config_run_result = {
        "config_name": config["name"],
        "beta": temp_beta_val,
        "hidden_layers": current_hidden_layers,
        "hidden_width": current_hidden_width,
        "activation": config["ACTIVATION_NAME"],
        "final_loss": current_epoch_avg_loss,
        "ntk_evolution_ Kxx_at_fixed_point": ntk_evolution_for_this_config
    }
    results_ntk_during_full_training.append(config_run_result)
    print(f"  Finished config {config['name']} in {time.time() - config_start_time:.2f}s. Final Loss: {current_epoch_avg_loss:.6f}")

# --- Convert to DataFrame and Display ---
ntk_full_training_df = pd.DataFrame(results_ntk_during_full_training)
print("\n--- Summary of NTK Evolution During Full Training ---")
display(ntk_full_training_df)


if not os.path.exists(output_directory):
    os.makedirs(output_directory)
    print(f"Created directory: {output_directory}")

base_filename = "gelu-ntk-during-training-25-05-01.dill.pkl"
pickle_filename = os.path.join(output_directory, base_filename)

ntk_full_training_df = pd.DataFrame(results_ntk_during_full_training)

with open(pickle_filename, 'wb') as f:
    dill.dump(ntk_full_training_df, f)
print(f"Results saved to {pickle_filename}")





base_filename = "sigmoid-ntk-during-training.dill.pkl"
pickle_filename = os.path.join(output_directory, base_filename)

loaded_results_df = None
try:
    with open(pickle_filename, 'rb') as f:
        loaded_results_df = dill.load(f)
except Exception as e:
    print(e)

ntk_full_training_df = loaded_results_df
# display(ntk_full_training_df)








configs_to_plot = []
sorted_df_for_selection = ntk_full_training_df.dropna(subset=['beta']).sort_values(by='beta')
if len(sorted_df_for_selection) > 0:
    for i in range(7):
        configs_to_plot.append(sorted_df_for_selection.iloc[i]['config_name'])
if len(sorted_df_for_selection) > 2:
    configs_to_plot.append(sorted_df_for_selection.iloc[len(sorted_df_for_selection)//2]['config_name'])
    configs_to_plot.append(sorted_df_for_selection.iloc[-1]['config_name'])
elif len(sorted_df_for_selection) > 1: 
    configs_to_plot.append(sorted_df_for_selection.iloc[-1]['config_name'])

configs_to_plot = list(set(configs_to_plot))

print(f"Selected configurations for epoch-wise K(x,x) plots: {configs_to_plot}")

plt.figure(figsize=(12, 7))
plot_count = 0
for config_name_to_plot in configs_to_plot:
    # Find the row for this config
    config_row_series = ntk_full_training_df[ntk_full_training_df['config_name'] == config_name_to_plot]
    if config_row_series.empty:
        print(f"Warning: Config '{config_name_to_plot}' not found in results.")
        continue
    
    config_data = config_row_series.iloc[0]
    
    evolution_data = config_data.get("ntk_evolution_ Kxx_at_fixed_point", [])
    if not evolution_data or not isinstance(evolution_data, list):
        print(f"No NTK evolution data or invalid format for {config_name_to_plot}")
        continue
        
    df_evo = pd.DataFrame(evolution_data)
    df_evo = df_evo.dropna(subset=['epoch', 'k_xx']) 

    if df_evo.empty or 'k_xx' not in df_evo.columns or 'epoch' not in df_evo.columns:
        print(f"Not enough valid data points for {config_name_to_plot} after dropping NaNs or missing columns.")
        continue
    
    # Normalize K(x,x) by its initial value (at epoch 0) for better comparison across different scales
    k_initial_series = df_evo[df_evo['epoch'] == 0]['k_xx']
    if not k_initial_series.empty:
        k_initial = k_initial_series.iloc[0]
        if k_initial != 0 and not pd.isna(k_initial):
            df_evo['k_xx_normalized'] = df_evo['k_xx'] / k_initial
            plt.plot(df_evo["epoch"], df_evo["k_xx_normalized"], marker='.', linestyle='-',
                     label=f"{config_data.get('config_name', 'Unknown')} (Beta: {config_data.get('beta', 'N/A'):.3f})")
            plot_count += 1
        else:
            print(f"Initial K(x,x) is zero or NaN for {config_name_to_plot}, plotting raw K(x,x).")
            plt.plot(df_evo["epoch"], df_evo["k_xx"], marker='.', linestyle='-',
                     label=f"{config_name_to_plot} (Beta: {config_data.get('beta', 'N/A'):.3f}) - Raw")
            plot_count += 1

    else:
        print(f"No K(x,x) data at epoch 0 for {config_name_to_plot}, plotting raw K(x,x).")
        plt.plot(df_evo["epoch"], df_evo["k_xx"], marker='.', linestyle='-',
                 label=f"{config_name_to_plot} (Beta: {config_data.get('beta', 'N/A'):.3f}) - Raw")
        plot_count +=1


plt.xlabel("Epoch (SGD Step during full training)")
plt.ylabel("Normalized K(x,x) at fixed point (K_epoch / K_0) or Raw K(x,x)")
plt.title(f"NTK Diagonal K(x,x) Evolution During Training (log scale)")
plt.yscale('log')
plt.legend(bbox_to_anchor=(1.03, 1), loc='upper left', title="Configuration")
plt.grid(True, which="both", linestyle=':')
plt.tight_layout(rect=[0, 0, 0.9, 1])
plt.show()
